# LLM角色扮演評估程式 - 實作報告與問題分析

## 專案概述
本程式用於評估大型語言模型（LLM）的角色扮演能力，支援Ollama和OpenRouter兩種服務提供者，測試三款指定模型：gpt-oss:20b、qwen2.5:14b、mistral-nemo:12b。

## 已完成的工作

### 1. 程式架構設計
```
llm_roleplay_evaluator/
├── config/           # 環境變數和設定管理
├── providers/        # LLM服務提供者（Ollama/OpenRouter）
├── prompts/          # 角色扮演提示詞範例
├── evaluator/        # 測試執行器和評分系統
├── data/results/     # 測試結果儲存
└── 主程式和設定檔案
```

### 2. 核心功能實現

#### 2.1 雙服務提供者支援
- **Ollama客戶端**：完整實現本地API整合
- **OpenRouter客戶端**：完整實現線上API整合
- 統一的基礎介面設計，易於擴充

#### 2.2 角色扮演提示詞設計（已改進）
設計了四種不同人格特質的一般人角色：
- **陳伯伯**：70歲樂觀退休教師
- **小李**：28歲焦慮年輕工程師  
- **王先生**：45歲自信中年主管
- **小美**：22歲創意藝術學生

每個角色都有：
- 詳細的背景設定和性格特質
- 具體的說話風格要求
- 強制性角色扮演指令

#### 2.3 自動化評分系統（已增強）
三維度評分標準：
- **角色一致性** (40%)：第一人稱使用、角色特質匹配、AI表述檢測
- **流暢度** (30%)：句子結構、回應長度、語法檢查
- **上下文連貫性** (30%)：問題回應、主題一致性

#### 2.4 統一測試框架
- 多輪對話測試執行
- 即時進度顯示和結果輸出
- JSON格式結果持久化儲存

### 3. 技術特色
- 模組化設計，易於維護和擴充
- 非同步處理提升效能
- 完善的錯誤處理和健康檢查
- 配置驅動的靈活設定

## 發現並修正的問題

### 問題1：LLM以AI角度回答（已修正）
**根本原因**：對話歷史處理錯誤，導致角色扮演指令被稀釋

**修正方案**：
- 在 `test_runner.py` 中新增 `_build_roleplay_messages()` 方法
- 確保每輪對話都重新包含系統提示詞
- 只保留使用者與助理的對話歷史，排除重複的系統訊息

**修正前**：
```python
messages = self.llm_client._build_messages(
    system_prompt, user_message, conversation_history  # 包含所有歷史訊息
)
```

**修正後**：
```python
messages = self._build_roleplay_messages(
    system_prompt, user_message, conversation_history  # 過濾後的歷史訊息
)
```

### 問題2：角色扮演提示詞不夠強制性（已改進）
**問題描述**：初始提示詞設計過於溫和，LLM容易忽略角色扮演要求

**改進方案**：
- 使用「【重要指令】」開頭強調強制性
- 明確列出5項嚴格要求
- 加入具體的角色姓名和背景細節
- 使用「請記住：你現在就是XXX！」強化角色認同

### 問題3：評分系統檢測不足（已增強）
**問題描述**：初始評分無法有效檢測AI式回答

**增強方案**：
- 新增第一人稱使用檢查（30%權重）
- 加入AI表述檢測和懲罰機制
- 增強角色特質匹配檢測

## 潛在問題與注意事項

### 1. 服務連線問題
- **Ollama**：需要確保本地服務正常運行在 http://localhost:11434
- **OpenRouter**：需要有效的API金鑰和網路連線

### 2. 模型相容性
- 不同模型對角色扮演的支援程度可能不同
- 某些模型可能更傾向於使用AI身份回答
- 建議測試前確認模型的中文角色扮演能力

### 3. 評分系統限制
- 自動評分主要基於文字分析，可能無法完全替代人工評估
- 角色一致性檢測主要依賴關鍵詞和語義相似度
- 對於細微的語氣和風格差異，人工評估更準確

### 4. 提示詞有效性
- 角色扮演效果高度依賴提示詞設計質量
- 可能需要根據具體模型調整提示詞強度和細節
- 建議進行小規模測試後再進行全面評估

## 使用指南

### 快速開始
1. 安裝依賴：`pip install -r requirements.txt`
2. 設定環境變數：複製 `.env.example` 為 `.env`
3. 執行測試：`python main.py --scenario optimistic_elderly`

### 測試命令
```bash
# 列出可用場景
python main.py --list-scenarios

# 單一場景測試
python main.py --scenario anxious_young_professional --rounds 3

# 全面測試
python main.py --comprehensive
```

### 結果解讀
- 總體分數 > 0.8：優秀的角色扮演能力
- 總體分數 0.6-0.8：良好的角色扮演能力  
- 總體分數 < 0.6：需要改進的角色扮演能力

## 後續優化建議

1. **增加更多測試場景**：擴充不同年齡、職業、性格的角色
2. **改進評分算法**：加入更細緻的語言風格分析
3. **支援批量測試**：同時測試多個模型和場景
4. **視覺化報告**：生成圖表化的測試結果報告
5. **人工評估整合**：結合人工評分提高評估準確性

## 結論
本程式已實現完整的LLM角色扮演評估功能，並修正了關鍵的技術問題。經過改進的提示詞設計和訊息處理流程，應該能夠有效測試LLM的角色扮演能力。建議在使用前進行小規模測試，確認設定正確後再進行全面評估。